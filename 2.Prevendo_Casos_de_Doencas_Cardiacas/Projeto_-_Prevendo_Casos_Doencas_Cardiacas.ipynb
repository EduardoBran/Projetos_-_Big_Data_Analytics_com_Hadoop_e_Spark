{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af931690",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Projeto - Prevendo Casos de Doenças Cardíacas</center></span>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Contexto\n",
    "\n",
    "<br>\n",
    "\n",
    "Este projeto tem como objetivo prever a ocorrência de doenças cardíacas usando ferramentas de **Big Data**, como **Hive** e **Spark**, e aplicando modelos preditivos baseados em **Machine Learning**.\n",
    "\n",
    "<br>\n",
    "\n",
    "### O que é uma doença cardíaca?\n",
    "\n",
    "Doença cardíaca é um termo geral para designar diversas condições médicas crônicas ou agudas que afetam um ou mais componentes do coração.\n",
    "\n",
    "- O coração está posicionado no mediastino, dois terços para a esquerda do centro do corpo humano.\n",
    "- É um órgão muscular, do tamanho de um punho, que bombeia sangue através do sistema cardiovascular.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Estruturas do coração:\n",
    "1. **Átrio direito**: recebe sangue das veias e bombeia para o ventrículo direito.\n",
    "2. **Ventrículo direito**: bombeia sangue para os pulmões, onde é oxigenado.\n",
    "3. **Átrio esquerdo**: recebe sangue oxigenado dos pulmões e bombeia para o ventrículo esquerdo.\n",
    "4. **Ventrículo esquerdo**: bombeia o sangue oxigenado para o corpo. Suas contrações criam a pressão arterial.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Outras partes importantes:\n",
    "- **Artérias coronárias**: fornecem sangue rico em oxigênio ao músculo cardíaco.\n",
    "- **Pericárdio**: saco que envolve o coração.\n",
    "- **Sistema nervoso**: regula contração e relaxamento do coração.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Doenças cardíacas comuns:\n",
    "- Angina (instável e estável)\n",
    "- Arritmia cardíaca\n",
    "- Artrose\n",
    "- Aterosclerose (doença coronária)\n",
    "- Arteriosclerose\n",
    "- Cardiomiopatia\n",
    "- Cardiopatia congênita\n",
    "- Doença arterial periférica\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Diagnóstico e Exames\n",
    "\n",
    "Exames comuns para diagnosticar ou monitorar doenças cardíacas:\n",
    "- **Eletrocardiograma**\n",
    "- **Ecocardiograma**\n",
    "- **Teste ergométrico**\n",
    "- **Cateterismo cardíaco**\n",
    "- **Holter 24 horas**\n",
    "- **Monitor cardíaco portátil**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Machine Learning para Previsão\n",
    "\n",
    "<br>\n",
    "\n",
    "> Modelos preditivos têm sido usados para prever a ocorrência de doenças cardíacas por meio de **algoritmos de classificação**. Esses modelos utilizam dados coletados de pacientes para prever doenças, incluindo cardíacas.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "O dataset fornecido `pacientes.csv` contém dados de pacientes idosos de um hospital. As colunas são:\n",
    "\n",
    "| Coluna                | Descrição                                                                 |\n",
    "|-----------------------|---------------------------------------------------------------------------|\n",
    "| **ID**                | ID único para cada registro                                              |\n",
    "| **Idade**             | Idade do paciente                                                       |\n",
    "| **Sexo**              | Sexo do paciente: `0` - Feminino, `1` - Masculino                        |\n",
    "| **Pressão Sanguínea** | Pressão sanguínea medida                                                 |\n",
    "| **Colesterol**        | Colesterol medido                                                       |\n",
    "| **Açúcar no Sangue**  | Nível de açúcar: `0` - <= 120 mg/dl, `1` - > 120 mg/dl                   |\n",
    "| **ECG**               | Resultados do ECG: `0` - Normal, `1` - Alguma anomalia, `2` - Presente  |\n",
    "| **Batimentos Cardíacos** | Valor máximo medido                                                   |\n",
    "| **Doença**            | Paciente tem doença cardíaca: `0` - Não, `1` - Sim                      |\n",
    "\n",
    "#### A variável **Doença** é a **`target`**. Todas as outras são **variáveis preditoras**.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exemplo do Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "| ID   | Idade | Sexo | Pressão Sanguínea | Colesterol | Açúcar no Sangue | ECG | Batimentos Cardíacos | Doença |\n",
    "|------|-------|------|-------------------|------------|-------------------|-----|-----------------------|--------|\n",
    "| 1001 | 63    | 1    | 145               | 233        | 1                 | 2   | 150                   | 0      |\n",
    "| 1002 | 67    | 1    | 160               | 286        | 0                 | 2   | 108                   | 1      |\n",
    "| 1003 | 69    | 1    | 145               | 235        | 1                 | 2   | 129                   | 0      |\n",
    "| 1004 | 68    | 1    | 120               | 229        | 0                 | 1   | 110                   | 0      |\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Tarefa do Cientista de Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "Criar um modelo preditivo que, usando as **variáveis preditoras**, seja capaz de prever, a partir de novos dados, se um paciente pode desenvolver doenças cardíacas.\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da847c8",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Iniciando o Projeto</center></span>\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "# Etapa 1. - Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "Ná **máquina virtual** executar os comandos abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1 Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "- **1.2 Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "- **1.3 Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```  \n",
    "<br>\n",
    "\n",
    "- **1.4 Verificar o Status do Safe Mode**:\n",
    "   ```bash\n",
    "   hdfs dfsadmin -safemode get\n",
    "   ```\n",
    "  - **1.4.1 Se o Safe Mode estiver ativado, forçar a saída**:\n",
    "  ```bash\n",
    "  hdfs dfsadmin -safemode leave\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.5 Criando diretório no HDFS**:\n",
    "   ```bash\n",
    "   hdfs dfs -mkdir /user/projetos\n",
    "   hdfs dfs -mkdir /user/projetos/projeto_prev_doenca_cardiaca\n",
    "   hdfs dfs -mkdir /user/projetos/projeto_prev_doenca_cardiaca/datasets\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "- **1.6 Define permissões amplas para evitar problemas de acesso**:\n",
    "   ```bash\n",
    "   hdfs dfs -chmod 777 /user/projetos/projeto_prev_doenca_cardiaca\n",
    "   ```\n",
    "   \n",
    "<br>\n",
    "\n",
    "- **1.7 Copiar o arquivo para o HDFS**:\n",
    "    ```bash\n",
    "    hdfs dfs -copyFromLocal /home/hadoop/Documents/Datasets/pacientes.csv /user/projetos/projeto_prev_doenca_cardiaca/datasets/\n",
    "    \n",
    "    hdfs dfs -ls /user/projetos/projeto_prev_doenca_cardiaca/datasets\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "- **1.8 Acessar o console do Hive**:\n",
    "   ```bash\n",
    "   hive\n",
    "   ```\n",
    "- **1.9 Verificar bancos de dados existentes**:\n",
    "   ```bash\n",
    "   show databases;\n",
    "   ```\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "# Etapa 2. - Carregando o Dataset no Hive e Visualizando os Dados com SQL\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa, criamos um banco de dados e uma tabela no **Hive** para armazenar os dados dos pacientes. Em seguida, carregamos os dados do arquivo `pacientes.csv` e realizamos algumas consultas iniciais.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Passos:\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.0 Inicie o console do `Hive` digitando no terminal**:\n",
    "\n",
    "```bash\n",
    "hive\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2.1 No console do `Hive`, escreva o comando para criar o Banco de Dados**:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Criando o banco de dados\n",
    "CREATE DATABASE usecase LOCATION '/user/projetos/projeto_prev_doenca_cardiaca/datasets';\n",
    "\n",
    "# Visualizando\n",
    "show databases;\n",
    "\n",
    "# Indicando que está ativo\n",
    "use usecase;\n",
    "\n",
    "# Verificar qual banco de dados está ativo\n",
    "SELECT current_database();\n",
    "```\n",
    "\n",
    "**Explicação**: Criamos um banco de dados chamado `usecase` para organizar os dados relacionados ao projeto. O diretório especificado na cláusula `LOCATION` no comando `CREATE DATABASE` deve ser o mesmo que você criou na **etapa 1.6** anteriormente no `HDFS`. Define e verifica o banco de dados `usecase` como o banco de dados ativo.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **2.2 Criação da tabela para armazenar os dados dos pacientes**:\n",
    "\n",
    "```bash\n",
    "# Cria a tabela\n",
    "CREATE TABLE pacientes (\n",
    "    ID INT, \n",
    "    IDADE INT, \n",
    "    SEXO INT, \n",
    "    PRESSAO_SANGUINEA INT, \n",
    "    COLESTEROL INT, \n",
    "    ACUCAR_SANGUE INT, \n",
    "    ECG INT, \n",
    "    BATIMENTOS INT, \n",
    "    DOENCA INT\n",
    ") \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "# Listar as tabelas do banco ativo:\n",
    "SHOW TABLES;\n",
    "\n",
    "# Informações sobre a tabela\n",
    "describe pacientes;\n",
    "```\n",
    "\n",
    "**Explicação**: Definimos a estrutura da tabela no **Hive** com as colunas correspondentes ao arquivo de entrada e especificamos que os dados estão separados por **vírgulas (CSV)**.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **2.3 Carregamento dos dados na tabela**:\n",
    "\n",
    "```sql\n",
    "LOAD DATA LOCAL INPATH '/home/hadoop/Documents/Datasets/pacientes.csv' OVERWRITE INTO TABLE pacientes;\n",
    "\n",
    "LOAD DATA INPATH '/user/projetos/projeto_prev_doenca_cardiaca/datasets/pacientes.csv' INTO TABLE pacientes;\n",
    "\n",
    "```\n",
    "\n",
    "**Explicação**: Carregamos os dados do arquivo `pacientes.csv` para a tabela **pacientes** localmente ou a partir do HDFS.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **2.4 Verificação da quantidade de registros na tabela**:\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) FROM pacientes;\n",
    "```\n",
    "\n",
    "**Explicação**: Contamos o número de registros carregados para garantir que os dados foram importados corretamente.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **2.5 Consultando resumo dos dados agrupados pela presença de doenças cardíacas**:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    doenca, \n",
    "    COUNT(*) AS total_pacientes, \n",
    "    AVG(idade) AS idade_media, \n",
    "    AVG(pressao_sanguinea) AS pressao_media, \n",
    "    AVG(colesterol) AS colesterol_medio, \n",
    "    AVG(acucar_sangue) AS acucar_medio, \n",
    "    AVG(batimentos) AS batimentos_medios \n",
    "FROM pacientes \n",
    "GROUP BY doenca;\n",
    "\n",
    "exit;\n",
    "```\n",
    "\n",
    "**Explicação**: Calculamos estatísticas descritivas (média e contagem) para identificar padrões nos dados relacionados à presença ou ausência de doenças cardíacas.\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "# Etapa 3 - Análise Exploratória e Transformação com Spark\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa, utilizamos o **Spark** para realizar a análise exploratória e transformação dos dados, preparando-os para a modelagem preditiva. Esta etapa envolve o **carregamento dos dados do HDFS**, **a inspeção básica dos dados** e a **criação de colunas categorizadas para facilitar a análise**.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3.0 Acessar o terminal do Spark**:\n",
    "\n",
    "```bash\n",
    "spark-shell\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Após entrar no terminal do **Spark** digitar linha por linha os comandos abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```scala\n",
    "// 3.1 Importar bibliotecas necessárias\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// 3.2 Criar SparkSession\n",
    "val spark = SparkSession.builder.appName(\"HeartDiseaseAnalysis\").getOrCreate()\n",
    "\n",
    "// 3.3 Carregar os dados do HDFS\n",
    "val df = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/datasets/pacientes.csv\").toDF(\"ID\", \"IDADE\", \"SEXO\", \"PRESSAO_SANGUINEA\", \"COLESTEROL\", \"ACUCAR_SANGUE\", \"ECG\", \"BATIMENTOS\", \"DOENCA\")\n",
    "\n",
    "// 3.4 Visualizar as primeiras linhas dos dados\n",
    "df.show(5)\n",
    "\n",
    "// 3.5 Exibir o esquema (colunas e tipos de dados)\n",
    "df.printSchema()\n",
    "\n",
    "// 3.6 Estatísticas descritivas gerais\n",
    "df.describe().show()\n",
    "\n",
    "// 3.7 Agrupar dados pela presença de doenças e calcular médias\n",
    "val groupedData = df.groupBy(\"DOENCA\").agg(avg(\"IDADE\").alias(\"IDADE_MEDIA\"), avg(\"PRESSAO_SANGUINEA\").alias(\"PRESSAO_MEDIA\"), avg(\"COLESTEROL\").alias(\"COLESTEROL_MEDIO\"), avg(\"BATIMENTOS\").alias(\"BATIMENTOS_MEDIOS\")) \n",
    "\n",
    "groupedData.show()\n",
    "\n",
    "// 3.8 Criar colunas categorizadas (faixas para variáveis contínuas)\n",
    "val dfWithCategories = df.withColumn(\"AgeRange\", ceil(col(\"IDADE\") / 10)).withColumn(\"BPRange\", ceil(col(\"PRESSAO_SANGUINEA\") / 25)).withColumn(\"CholRange\", ceil(col(\"COLESTEROL\") / 25))\n",
    "\n",
    "// 3.9 Exibir os dados com as novas colunas categorizadas\n",
    "dfWithCategories.show()\n",
    "\n",
    "// 3.10 Salvar os dados transformados no HDFS\n",
    "dfWithCategories.write.option(\"header\", \"true\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/datasets/processed_data\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "# Etapa 4 - Criação e Treinamento do Modelo Preditivo de Classificação com Spark MLlib\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa, utilizamos o **Spark MLlib** para preparar os dados e treinar um modelo de classificação. Começamos **criando vetores de características**, **dividimos os dados em treino e teste** e **utilizamos um classificador Random Forest para treinamento**. \n",
    "\n",
    "**Importante**: necessário ter executados passos 3.1 e 3.2 anteriormente.\n",
    "\n",
    "<br>\n",
    "\n",
    "```scala\n",
    "// 4.1 Importar bibliotecas necessárias para MLlib\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "// 4.2 Carregar os dados transformados do HDFS\n",
    "val processedData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/datasets/processed_data\")\n",
    "\n",
    "processedData.show(5)\n",
    "\n",
    "// 4.3 Criar um vetor de características para o modelo\n",
    "val dataWithFeatures = new VectorAssembler().setInputCols(Array(\"AgeRange\", \"BPRange\", \"CholRange\", \"ACUCAR_SANGUE\", \"ECG\")).setOutputCol(\"features\").transform(processedData)\n",
    "\n",
    "// 4.4 Dividir os dados em conjuntos de treino (70%) e teste (30%)\n",
    "val Array(trainingData, testData) = dataWithFeatures.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// 4.5 Criar o modelo de Random Forest\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"DOENCA\").setFeaturesCol(\"features\").setNumTrees(25)\n",
    "\n",
    "// 4.6 Treinar o modelo\n",
    "val model = rf.fit(trainingData)\n",
    "\n",
    "// 4.7 Fazer previsões com o conjunto de teste\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// 4.8 Exibir amostra das previsões\n",
    "predictions.select(\"features\", \"prediction\", \"DOENCA\").show()\n",
    "```\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "# Etapa 5 - Avaliando Desempenho do Modelo Iniciar, Otimizando o Modelo Preditivo e Avaliando Novamente\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta etapa, **avaliamos a performance do modelo inicial** e **buscamos otimizar os parâmetros para melhorar sua precisão**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```scala\n",
    "// 5.1 Avaliar a precisão do modelo com os dados de teste\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"DOENCA\").setMetricName(\"accuracy\")\n",
    "\n",
    "val initialAccuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "println(s\"Accuracy of the initial model: $initialAccuracy\")\n",
    "\n",
    "// 5.2 Ajustar hiperparâmetros (aumentar o número de árvores para 50)\n",
    "val rfOptimized = new RandomForestClassifier().setLabelCol(\"DOENCA\").setFeaturesCol(\"features\").setNumTrees(50)\n",
    "\n",
    "// 5.3 Treinar o modelo otimizado\n",
    "val optimizedModel = rfOptimized.fit(trainingData)\n",
    "\n",
    "// 5.4 Fazer previsões com o modelo otimizado\n",
    "val optimizedPredictions = optimizedModel.transform(testData)\n",
    "\n",
    "// 5.5 Avaliar a precisão do modelo otimizado\n",
    "val optimizedAccuracy = evaluator.evaluate(optimizedPredictions)\n",
    "\n",
    "println(s\"Accuracy of the optimized model: $optimizedAccuracy\")\n",
    "\n",
    "// 5.6 Salvar as previsões do modelo otimizado no HDFS\n",
    "optimizedPredictions.select(\"features\", \"prediction\", \"DOENCA\").write.option(\"header\", \"true\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/results/optimized_predictions\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para sair do terminal do spark, digite**:\n",
    "\n",
    "```bash\n",
    ":quit\n",
    "```\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## Executando o Projeto com Script Externo\n",
    "\n",
    "<br>\n",
    "\n",
    "- Vá até o terminal e digite `gedit HeartDiseaseAnalysis.scala`. cole o conteúdo abaixo e salve:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "object HeartDiseaseAnalysis {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    // 1. Criar SparkSession\n",
    "    val spark = SparkSession.builder\n",
    "      .appName(\"HeartDiseaseAnalysis\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    // 2. Carregar os dados do HDFS\n",
    "    val df = spark.read\n",
    "      .option(\"header\", \"false\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/datasets/pacientes.csv\")\n",
    "      .toDF(\"ID\", \"IDADE\", \"SEXO\", \"PRESSAO_SANGUINEA\", \"COLESTEROL\", \"ACUCAR_SANGUE\", \"ECG\", \"BATIMENTOS\", \"DOENCA\")\n",
    "\n",
    "    // 3. Estatísticas descritivas e categorização\n",
    "    df.describe().write.mode(\"overwrite\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/results/summary_stats\")\n",
    "    \n",
    "    val groupedData = df.groupBy(\"DOENCA\")\n",
    "      .agg(\n",
    "        avg(\"IDADE\").alias(\"IDADE_MEDIA\"),\n",
    "        avg(\"PRESSAO_SANGUINEA\").alias(\"PRESSAO_MEDIA\"),\n",
    "        avg(\"COLESTEROL\").alias(\"COLESTEROL_MEDIO\"),\n",
    "        avg(\"BATIMENTOS\").alias(\"BATIMENTOS_MEDIOS\")\n",
    "      )\n",
    "    groupedData.write.mode(\"overwrite\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/results/grouped_data\")\n",
    "\n",
    "    val dfWithCategories = df\n",
    "      .withColumn(\"AgeRange\", ceil(col(\"IDADE\") / 10))\n",
    "      .withColumn(\"BPRange\", ceil(col(\"PRESSAO_SANGUINEA\") / 25))\n",
    "      .withColumn(\"CholRange\", ceil(col(\"COLESTEROL\") / 25))\n",
    "\n",
    "    dfWithCategories.write.mode(\"overwrite\").csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/datasets/processed_data\")\n",
    "\n",
    "    // 4. Preparar dados para Random Forest\n",
    "    val processedData = spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/datasets/processed_data\")\n",
    "\n",
    "    val dataWithFeatures = new VectorAssembler()\n",
    "      .setInputCols(Array(\"AgeRange\", \"BPRange\", \"CholRange\", \"ACUCAR_SANGUE\", \"ECG\"))\n",
    "      .setOutputCol(\"features\")\n",
    "      .transform(processedData)\n",
    "\n",
    "    val Array(trainingData, testData) = dataWithFeatures.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "    // 5. Criar e treinar modelo inicial\n",
    "    val rf = new RandomForestClassifier()\n",
    "      .setLabelCol(\"DOENCA\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setNumTrees(25)\n",
    "\n",
    "    val model = rf.fit(trainingData)\n",
    "    val predictions = model.transform(testData)\n",
    "    predictions.select(\"features\", \"prediction\", \"DOENCA\")\n",
    "      .write\n",
    "      .mode(\"overwrite\")\n",
    "      .csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/results/initial_predictions\")\n",
    "\n",
    "    // 6. Avaliar modelo inicial\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"DOENCA\")\n",
    "      .setMetricName(\"accuracy\")\n",
    "\n",
    "    val initialAccuracy = evaluator.evaluate(predictions)\n",
    "    println(s\"Accuracy of the initial model: $initialAccuracy\")\n",
    "\n",
    "    // 7. Criar e treinar modelo otimizado\n",
    "    val rfOptimized = new RandomForestClassifier()\n",
    "      .setLabelCol(\"DOENCA\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setNumTrees(50)\n",
    "\n",
    "    val optimizedModel = rfOptimized.fit(trainingData)\n",
    "    val optimizedPredictions = optimizedModel.transform(testData)\n",
    "\n",
    "    // 8. Avaliar modelo otimizado\n",
    "    val optimizedAccuracy = evaluator.evaluate(optimizedPredictions)\n",
    "    println(s\"Accuracy of the optimized model: $optimizedAccuracy\")\n",
    "\n",
    "    optimizedPredictions.selectExpr(\"CAST(features AS STRING) AS features\", \"prediction\", \"DOENCA\")\n",
    "      .write\n",
    "      .mode(\"overwrite\")\n",
    "      .csv(\"hdfs://localhost:9000/user/projetos/projeto_prev_doenca_cardiaca/results/optimized_predictions\")\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Na sequência, no terminal do computador digite:\n",
    "\n",
    "```bash\n",
    "spark-shell -i /home/hadoop/Documents/Scripts/HeartDiseaseAnalysis.scala\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Após isso verifique os resultados no HDFS:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /user/projetos/projeto_prev_doenca_cardiaca/results\n",
    "```\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "### Visualize os Resultados no HDFS:\n",
    "\n",
    "<br>\n",
    "\n",
    "Digite no terminal:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /user/projetos/projeto_prev_doenca_cardiaca/results\n",
    "\n",
    "hdfs dfs -ls /user/projetos/projeto_prev_doenca_cardiaca/results/optimized_predictions\n",
    "\n",
    "hdfs dfs -cat /user/projetos/projeto_prev_doenca_cardiaca/results/optimized_predictions/part-00000-b5265274-19d5-4f98-8fb4-881ae4360cdd-c000.csv | head -10\n",
    "\n",
    "```\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "# <center><span style=\"font-size: 34px;color: black;\">Resumo do Projeto: Previsão de Doenças Cardíacas</center></span>\n",
    "\n",
    "---\n",
    "\n",
    "## **Resumo das Etapas**\n",
    "\n",
    "1. **Configuração do Ambiente no HDFS**:\n",
    "   - Diretórios criados no HDFS.\n",
    "   - Arquivo `pacientes.csv` carregado para o HDFS.\n",
    "\n",
    "2. **Configuração e Carregamento no Hive**:\n",
    "   - Banco de dados `usecase` criado no Hive.\n",
    "   - Tabela `pacientes` configurada e carregada com dados do HDFS.\n",
    "\n",
    "3. **Análise Exploratória e Transformação com Spark**:\n",
    "   - Dados carregados do HDFS no Spark.\n",
    "   - Novas colunas categorizadas criadas: `AgeRange`, `BPRange`, e `CholRange`.\n",
    "   - Dados transformados salvos no HDFS para modelagem preditiva.\n",
    "\n",
    "4. **Treinamento do Modelo com Spark MLlib**:\n",
    "   - Vetores de características gerados usando `VectorAssembler`.\n",
    "   - Modelo Random Forest treinado com 25 árvores.\n",
    "   - Previsões realizadas e armazenadas.\n",
    "\n",
    "5. **Avaliação e Otimização do Modelo**:\n",
    "   - Avaliação inicial do modelo com `MulticlassClassificationEvaluator` (Accuracy: 75.6%).\n",
    "   - Modelo otimizado com 50 árvores Random Forest.\n",
    "   - Resultados das previsões otimizadas armazenados no HDFS.\n",
    "\n",
    "6. **Visualização Final no HDFS**:\n",
    "   - Resultados verificados no HDFS com `hdfs dfs -cat` para amostras de previsões.\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "# Fim!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc3449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
