{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137a739a",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Projeto - Design De Um MapReduce com Spark e MRJob para os Gastos Totais por Cliente</center></span>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Análise de Vendas em Grandes Empresas\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Você já parou para pensar quantas vendas são realizadas por dia em grandes empresas como, por exemplo, **Amazon** ou **Walmart**?  \n",
    "Empresas que faturam bilhões vendendo os mais variados produtos para um grande número de clientes.\n",
    "\n",
    "E se você fosse contratado para um projeto em uma dessas empresas e seu primeiro trabalho fosse calcular o total de vendas por cliente?  \n",
    "Tarefa aparentemente simples. Sua primeira abordagem talvez fosse buscar o banco de dados transacional com as informações de vendas, cruzar os dados com o cadastro de clientes e obter o valor total gasto por cliente.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## O Problema\n",
    "\n",
    "Mas quantos clientes uma empresa como a Amazon possui?  \n",
    "E se a solicitação fosse para gerar o total gasto por cliente nos últimos **5 anos**, de modo a criar uma campanha personalizada para os clientes que tiveram os maiores gastos ao longo dos anos?\n",
    "\n",
    "Após alguma pesquisa, você poderia obter um dataset no seguinte formato:\n",
    "\n",
    "<br>\n",
    "\n",
    "| Código do cliente | Valor gasto em uma única compra |\n",
    "|-------------------|---------------------------------|\n",
    "| 1288              | 99.90                          |\n",
    "| 1029              | 349.12                         |\n",
    "| 1284              | 5.76                           |\n",
    "\n",
    "<br>\n",
    "\n",
    "Sua pesquisa identificou que todos os registros dos últimos **5 anos** geram um dataset com apenas duas colunas, mas **200 milhões de registros**. Definitivamente, esse não é um trabalho para um banco de dados relacional.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## A Solução\n",
    "\n",
    "Você precisa de uma ferramenta que possa **rapidamente processar os dados e retornar apenas um valor total por cliente**.  \n",
    "Você então decide criar um job de **MapReduce**. Com poucas linhas de código e usando a linguagem **Python**, você consegue gerar o resultado esperado.\n",
    "\n",
    "Mas ainda tem um problema: **Como processar esse job da forma mais rápida possível?**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Big Data: A Solução Ideal\n",
    "\n",
    "**Spark**/**Hadoop** é a solução ideal. Esse é um exemplo claro de projeto de **Big Data**.  \n",
    "Um grande volume de dados e tudo que você precisa é extrair uma simples informação, que poderá fazer toda a diferença na estratégia da empresa.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Objetivo\n",
    "\n",
    "<br>\n",
    "\n",
    "Seu trabalho agora é **criar um Job de MapReduce que processe o grande volume de dados para calcular o total de gastos por cliente**. O objetivo principal é consolidar os dados de várias transações para cada cliente, somando os valores de todas as compras realizadas nos últimos 5 anos. Esse resultado permitirá identificar os clientes com os maiores gastos, possibilitando a criação de campanhas personalizadas e estratégias de retenção.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Explicação do Objetivo\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **Entrada**: Um dataset com milhões de registros contendo o identificador do cliente e o valor gasto em cada transação.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Processo**:\n",
    "\n",
    "   - **Map**: Dividir os dados por cliente e preparar os valores para serem somados.\n",
    "   - **Reduce**: Consolidar os valores de todas as transações por cliente, calculando o total gasto por cada um.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Saída**: Um conjunto de dados com os identificadores de clientes e o total de gastos consolidado, no formato:\n",
    "\n",
    "```bash\n",
    "Id_Cliente, Gasto_Total\n",
    "```\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37954831",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Iniciando o Projeto</center></span>\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "# Etapa 1. - Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "Ná **máquina virtual** executar os comandos abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1 Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "- **1.2 Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "- **1.3 Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```  \n",
    "<br>\n",
    "\n",
    "- **1.4 Verificar o Status do Safe Mode**:\n",
    "   ```bash\n",
    "   hdfs dfsadmin -safemode get\n",
    "   ```\n",
    "  - **1.4.1 Se o Safe Mode estiver ativado, forçar a saída**:\n",
    "  ```bash\n",
    "  hdfs dfsadmin -safemode leave\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.5 Criando diretório no HDFS**:\n",
    "   ```bash\n",
    "   hdfs dfs -mkdir /user/projetos\n",
    "   hdfs dfs -mkdir /user/projetos/design_mapreduce_gastos_totais\n",
    "   hdfs dfs -mkdir /user/projetos/design_mapreduce_gastos_totais/datasets\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "- **1.6 Define permissões amplas para evitar problemas de acesso**:\n",
    "   ```bash\n",
    "   hdfs dfs -chmod 777 /user/projetos/design_mapreduce_gastos_totais\n",
    "   ```\n",
    "   \n",
    "<br>\n",
    "\n",
    "- **1.7 Copiar o arquivo para o HDFS**:\n",
    "    ```bash\n",
    "    hdfs dfs -copyFromLocal /home/hadoop/Documents/Datasets/gastos-cliente.csv /user/projetos/design_mapreduce_gastos_totais/datasets/\n",
    "    \n",
    "    hdfs dfs -ls /user/projetos/design_mapreduce_gastos_totais/datasets\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "- **1.8 Visualizar as primeiras linhas**:\n",
    "    ```bash\n",
    "    hdfs dfs -cat /user/projetos/design_mapreduce_gastos_totais/datasets/gastos-cliente.csv | head -n 10\n",
    "    ```\n",
    "    \n",
    "<br><br><br>\n",
    "\n",
    "# Etapa 2. - Criando o MapReduce (Utilizando MRJob)\n",
    "\n",
    "<br>\n",
    "\n",
    "Criar o código para MapReduce utilizando a biblioteca **MRJob**, que permite executar jobs MapReduce no Hadoop.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Versão 1\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRGastos_Cliente(MRJob):\n",
    "\tdef mapper(self, key, line):\n",
    "\t\tvalues = line.split(',')\n",
    "\n",
    "\t\t# Sabendo que o dataset contém 3 colunas sem cabeçalho representado por ID, ID_Cliente e Gasto_Total.\n",
    "\t\t# Vamos mapeas apenas ID_Cliente e Gasto_Total.\n",
    "\n",
    "\t\tid_cliente = values[1]\n",
    "\t\tgasto_total = float(values[2])  # Convertendo para somar no reducer\n",
    "\n",
    "\t\t# Emitir o par chave-valor\n",
    "\t\tyield id_cliente, gasto_total\n",
    "\n",
    "\n",
    "\tdef reducer(self, id_cliente, gastos):\n",
    "\n",
    "\t\t# Somando todas as ocorrências por cliente\n",
    "\t\tyield id_cliente, sum(gastos)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tMRGastos_Cliente.run()\n",
    "```\n",
    "\n",
    "#### Versão 2\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRGastos_Cliente(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        values = line.split(',')\n",
    "\n",
    "        # Sabendo que o dataset contém 3 colunas sem cabeçalho representado por ID, ID_Cliente e Gasto_Total.\n",
    "        # Vamos mapear apenas ID_Cliente e Gasto_Total.\n",
    "\n",
    "        id_cliente = values[1]\n",
    "        gasto_total = float(values[2])  # Convertendo para somar no reducer\n",
    "\n",
    "        # Emitir o par chave-valor\n",
    "        yield id_cliente, gasto_total\n",
    "\n",
    "    def reducer(self, id_cliente, gastos):\n",
    "        # Somando todas as ocorrências por cliente\n",
    "        total_gasto = sum(gastos)\n",
    "        # Emitir o resultado (todos os clientes e seus gastos totais)\n",
    "        yield None, (total_gasto, id_cliente)\n",
    "\n",
    "    def reducer_final(self, _, id_cliente_gasto):\n",
    "        # Ordenando os resultados por gasto e pegando os 15 maiores\n",
    "        top_15 = sorted(id_cliente_gasto, reverse=True, key=lambda x: x[0])[:15]\n",
    "\n",
    "        # Exibir no terminal somente os 15 maiores\n",
    "        for gasto, id_cliente in top_15:\n",
    "            print(f\"Cliente {id_cliente} gastou {gasto}\")\n",
    "\n",
    "        # Salvar todos os resultados no HDFS\n",
    "        #with open('/mnt/data/resultados_completos.txt', 'w') as f:\n",
    "            #for gasto, id_cliente in id_cliente_gasto:\n",
    "                #f.write(f\"Cliente {id_cliente} gastou {gasto}\\n\")\n",
    "\n",
    "    def steps(self):\n",
    "        # Definir os passos para o job\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_final)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRGastos_Cliente.run()\n",
    "```\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Etapa 3. - Executando o MapReduce (Utilizando MRJob)\n",
    "\n",
    "<br>\n",
    "\n",
    "Ir ao terminal e digitar o comando:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "python MapReduceGastosCliente.py hdfs:////user/projetos/design_mapreduce_gastos_totais/datasets/gastos-cliente.csv  -r hadoop --python-bin ~/.conda/envs/py397/bin/python\n",
    "```\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Etapa 4. - Criando o MapReduce (Utilizando Spark)\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora vamos criar o código para MapReduce utilizando **PySpark**. Este código pode ser executado com o comando `spark-submit`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Versão 1\n",
    "```python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Configuração do Spark Context, pois o job será executado via linha de comando com o spark-submit\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"GastosPorCliente\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Função de mapeamento que separa cada um dos campos no dataset\n",
    "def MapCliente(line):\n",
    "    campos = line.split(',')\n",
    "    return (int(campos[1]), float(campos[2]))  # Retorna uma tupla (ID_Cliente, Gasto_Total)\n",
    "\n",
    "# Leitura do dataset a partir do HDFS\n",
    "input = sc.textFile(\"hdfs://localhost:9000/user/projetos/design_mapreduce_gastos_totais/datasets/gastos-cliente.csv\")\n",
    "\n",
    "mappedInput = input.map(MapCliente)\n",
    "\n",
    "# Operação de redução por chave para calcular o total gasto por cliente\n",
    "totalPorCliente = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Imprime o resultado\n",
    "resultados = totalPorCliente.collect()\n",
    "for resultado in resultados:\n",
    "    print(resultado)\n",
    "\n",
    "# Salvar os resultados completos no HDFS\n",
    "totalPorCliente.saveAsTextFile(\"hdfs://localhost:9000/user/projetos/design_mapreduce_gastos_totais/saida_resultados_completos\")\n",
    "```\n",
    "\n",
    "### Explicação do código:\n",
    "\n",
    "1. **SparkContext**: Configura o ambiente do Spark, definindo a aplicação como \"GastosPorCliente\". A opção `local` significa que o job será executado localmente (não em um cluster distribuído).\n",
    "\n",
    "2. **MapCliente**: Função de mapeamento que transforma cada linha do arquivo CSV em uma tupla contendo o `ID_Cliente` (inteiro) e o `Gasto_Total` (float).\n",
    "\n",
    "3. **Leitura do Dataset**: Utiliza o Spark para ler o arquivo CSV do HDFS utilizando `sc.textFile(\"hdfs://...\")`.\n",
    "\n",
    "4. **reduceByKey**: A operação de redução soma os gastos por cliente.\n",
    "\n",
    "5. **Impressão e Salvamento**: Os resultados são coletados e impressos no terminal. Além disso, os resultados completos (todos os clientes) são salvos no HDFS.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Etapa 5. - Executando o MapReduce (Utilizando Spark)\n",
    "\n",
    "<br>\n",
    "\n",
    "Ir ao terminal e digitar o comando:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "spark-submit --master local --deploy-mode client --conf spark.executor.memory=2g --conf spark.driver.memory=2g MapReduceGastosClientes_Spark.py\n",
    "```\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "# Resumo do Projeto: Design de um MapReduce com Spark para os Gastos Totais por Cliente\n",
    "\n",
    "## Objetivo do Projeto\n",
    "O objetivo principal deste projeto foi criar um job de MapReduce utilizando **PySpark** para calcular o total de gastos por cliente em um grande conjunto de dados. A empresa fictícia **Amazon** ou **Walmart** foi utilizada como exemplo para ilustrar a necessidade de processar milhões de registros de vendas para identificar os clientes que mais gastaram ao longo de um período de cinco anos.\n",
    "\n",
    "O job foi projetado para:\n",
    "\n",
    "- **Entrada**: Um dataset com milhões de registros, onde cada linha representa uma transação de compra contendo o **ID do Cliente** e o **Valor Gasto**.\n",
    "- **Processo**: Utilizou-se a abordagem de MapReduce para:\n",
    "  - **Map**: Separar os dados por cliente, atribuindo os valores de compras a cada cliente.\n",
    "  - **Reduce**: Somar os valores de todas as compras feitas por um cliente, obtendo o total gasto por ele.\n",
    "- **Saída**: Um conjunto de dados com o **ID do Cliente** e o **Gasto Total** consolidado.\n",
    "\n",
    "---\n",
    "\n",
    "## Passos do Projeto\n",
    "\n",
    "### Preparação do Ambiente e Iniciação dos Serviços\n",
    "\n",
    "- O ambiente foi configurado utilizando **Hadoop** e **Spark**. A máquina virtual foi configurada para rodar os serviços necessários (**HDFS**, **YARN**, **Spark**).\n",
    "- Foram criados diretórios no **HDFS** para armazenar os dados e o resultado final. Os arquivos de dados foram copiados para o HDFS, e a segurança de acesso foi configurada para garantir que o job fosse executado corretamente.\n",
    "\n",
    "### MapReduce com MRJob (Versões 1 e 2)\n",
    "\n",
    "Inicialmente, a solução foi implementada usando a biblioteca **MRJob**, que permite a execução de jobs MapReduce no Hadoop. Duas versões do código foram criadas:\n",
    "\n",
    "- **Versão 1**: Utilizando a função `mapper` para mapear o **ID do Cliente** e o **Gasto Total**, e a função `reducer` para somar os gastos por cliente.\n",
    "- **Versão 2**: Incluindo um segundo estágio no job para selecionar e exibir os 15 clientes com maior gasto total, além de salvar todos os resultados no HDFS.\n",
    "\n",
    "### MapReduce com Spark (PySpark)\n",
    "\n",
    "Após a implementação inicial com MRJob, a solução foi otimizada utilizando **PySpark**, que oferece maior flexibilidade e desempenho para processamento distribuído.\n",
    "\n",
    "- **PySpark** foi configurado e o código de MapReduce foi criado utilizando a função `map` para dividir os dados e `reduceByKey` para somar os valores.\n",
    "- O job foi executado localmente com o comando `spark-submit` e os resultados foram salvos no **HDFS**, conforme especificado.\n",
    "\n",
    "### Execução do Job\n",
    "\n",
    "- O job foi executado com sucesso em ambas as abordagens (**MRJob** e **PySpark**). O resultado da execução foi a soma dos gastos por cliente, que foi coletada e salva em um diretório do **HDFS**.\n",
    "- Durante a execução, a configuração do **HDFS** e do **Spark** foi ajustada para garantir que a quantidade de memória e a configuração de recursos fosse adequada para processar os dados grandes sem problemas de desempenho.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "O projeto foi concluído com sucesso, e o objetivo de calcular os gastos totais por cliente foi alcançado utilizando **MapReduce** com **PySpark**. A solução foi capaz de processar grandes volumes de dados, somando os valores de transações de forma eficiente, e foi escalável o suficiente para rodar em um cluster Hadoop se necessário.\n",
    "\n",
    "O projeto demonstrou como **Big Data** e **MapReduce** são poderosas ferramentas para resolver problemas de análise de grandes volumes de dados, como o cálculo de gastos totais de clientes em empresas de grande porte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e564d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
